# 【大规模语言模型】学习笔记

作者：wallace-lai <br/>
发布：2024-05-16 <br/>
更新：2024-05-27 <br/>

## 第一章

## 第二章

### 1. 自然语言处理有哪些基础任务？

自然语言处理NLP是一个相当广泛的领域，包含众多的基础任务，以下是一些主要的NLP基础任务：

（1）**分词**：将文本切分为有意义的单词、符号或者子词序列

（2）**词性标注**：识别文本中每个单词的词性（如名词、动词、形容词等）

（3）**命名实体识别**：识别文本中具有特定意义的实体，比如人名、地名、机构名、日期、时间等

（4）**句法分析**：分析句子的句法结构，包括短语结构树和依存关系图

（5）**语义角色标注**：识别句子中谓词（动词或形容词）的论元，并标注它们之间的语义关系

（6）**情感分析**：判断文本表达的情感倾向，如正面、负面或者中性

（7）**文本分类**：将文本分配给一个或者多个预定义的类别，如垃圾邮件检测、主题分类等

（8）**文本相似度计算**：计算两段文本之间的相似度或差异度

（9）**问答系统**：构建能够自动回答用户问题的系统

（10）**机器翻译**：将一种语言的文本自动翻译成另一种语言

（11）**对话系统**：构建可以与用户进行自然语言交互的系统，如聊天机器人

### 2. 什么是语言模型？它有什么作用？
语言模型用于计算一个文本序列（例如句子和段落）的概率分布。它常常被用来评估一个文本序列在语言学上是否合理，或者用来预测文本序列中下一个可能出现的词。

更加具体一点来说是，语言模型的目标是建模自然语言的概率分布。词汇表$\mathbb{V}$上的语言模型，由函数$P(w_1w_2...w_m)$来表示，其可以形式化地构建为词序列$w_1w_2...w_n$的概率分布，表示词序列$w_1w_2...w_n$作为一个句子出现的可能性大小。

### 3. 什么是注意力机制？
**（1）什么是注意力机制？**
注意力机制是深度学习中的一个重要概念，它借鉴了人类的注意力思维方式。在认知科学中，由于信息处理瓶颈，人类会选择性地关注所有信息的一部分，同时忽略其他可见的信息。这种机制通常被称为注意力机制。

在深度学习中，注意力机制被用于模拟人类的这种注意力行为，使得模型能够对输入数据的不同部分赋予不同的权重，从而更加关注重要的信息。具体来说，注意力机制通常包括以下几个步骤：

1）**计算注意力权重**：模型首先会计算输入数据中每个部分（如单词、像素等）的注意力权重。这个权重通常是根据模型学习到的某种度量或者相似度函数来计算的，反映了该部分与当前任务的相关性。

2）**加权求和**：然后，模型会将输入数据的每个部分与其对应的注意力权重相乘，并进行加权求和。这样，模型就能够根据输入数据的不同部分的重要性来生成一个加权表示。

3）**使用加权表示**：最后，模型会使用这个加权表示来进行后续的计算或预测。由于加权表示中包含了输入数据中不同部分的重要性信息，因此模型能够更加关注重要的信息，从而提高性能。

**（2）Transformer架构中，注意力机制是如何对序列中任意两个单词之间的依赖关系进行建模的？**

**（3）Transformer架构中，什么是多头注意力机制？**

【pending】